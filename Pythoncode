Creating a neural network from scratch using Python and NumPy to classify handwritten digits from the MNIST dataset involves several steps. Below is a step-by-step guide to accomplish this task:

1. **Load the MNIST Dataset**
2. **Preprocess the Data**
3. **Initialize the Neural Network Parameters**
4. **Define Forward and Backward Propagation**
5. **Implement the Training Loop**
6. **Evaluate the Model**

Let's go through each step in detail.

### Step 1: Load the MNIST Dataset

First, we need to load the MNIST dataset. We can use the `tensorflow.keras.datasets` module to download and load the data.

```python
import numpy as np
import tensorflow as tf

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# Normalize the input data
x_train = x_train / 255.0
x_test = x_test / 255.0
```

### Step 2: Preprocess the Data

We need to flatten the 28x28 images into 1D arrays of size 784 and convert the labels to one-hot encoded vectors.

```python
# Flatten the images
x_train = x_train.reshape(-1, 28*28)
x_test = x_test.reshape(-1, 28*28)

# One-hot encode the labels
y_train = np.eye(10)[y_train]
y_test = np.eye(10)[y_test]
```

### Step 3: Initialize the Neural Network Parameters

We'll create a simple neural network with one hidden layer.

```python
def initialize_parameters(input_size, hidden_size, output_size):
    np.random.seed(42)
    W1 = np.random.randn(input_size, hidden_size) * 0.01
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.randn(hidden_size, output_size) * 0.01
    b2 = np.zeros((1, output_size))
    
    return W1, b1, W2, b2
```

### Step 4: Define Forward and Backward Propagation

Forward propagation calculates the predictions, and backward propagation updates the parameters.

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = sigmoid(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    
    return Z1, A1, Z2, A2

def compute_cost(A2, Y):
    m = Y.shape[0]
    logprobs = -np.log(A2[range(m), np.argmax(Y, axis=1)])
    cost = np.sum(logprobs) / m
    return cost

def backward_propagation(X, Y, Z1, A1, A2, W1, W2):
    m = X.shape[0]
    
    dZ2 = A2 - Y
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m
    
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * sigmoid_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m
    
    return dW1, db1, dW2, db2
```

### Step 5: Implement the Training Loop

The training loop will involve iterating over the dataset and updating the parameters.

```python
def model(X_train, Y_train, X_test, Y_test, hidden_size, learning_rate, num_iterations):
    input_size = X_train.shape[1]
    output_size = Y_train.shape[1]
    
    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size, output_size)
    
    for i in range(num_iterations):
        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)
        
        cost = compute_cost(A2, Y_train)
        if i % 100 == 0:
            print(f"Cost after iteration {i}: {cost}")
        
        dW1, db1, dW2, db2 = backward_propagation(X_train, Y_train, Z1, A1, A2, W1, W2)
        
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
    
    return W1, b1, W2, b2
```

### Step 6: Evaluate the Model

We can now evaluate the model's performance on the test set.

```python
def predict(X, W1, b1, W2, b2):
    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)
    predictions = np.argmax(A2, axis=1)
    return predictions

def accuracy(predictions, labels):
    return np.mean(predictions == labels)

W1, b1, W2, b2 = model(x_train, y_train, x_test, y_test, hidden_size=64, learning_rate=0.1, num_iterations=1000)

train_predictions = predict(x_train, W1, b1, W2, b2)
test_predictions = predict(x_test, W1, b1, W2, b2)

print(f"Train accuracy: {accuracy(train_predictions, np.argmax(y_train, axis=1))}")
print(f"Test accuracy: {accuracy(test_predictions, np.argmax(y_test, axis=1))}")
```

This complete implementation will train a simple neural network on the MNIST dataset using only NumPy and evaluate its performance. You can adjust the `hidden_size`, `learning_rate`, and `num_iterations` parameters to improve the model's accuracy.....
